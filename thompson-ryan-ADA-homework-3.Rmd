---
title: "thompson-ryan-ADA-homework-3"
author: "KRT"
date: "4/23/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages, include = FALSE}
library(broom)
library(tidyverse)
library(car)
library(infer)
library(gridExtra)
```

# Challenge 1 

## Uploading Kamilar and Cooper.
```{r}

## Challenge 1

f <-"https://raw.githubusercontent.com/difiore/ADA-datasets/master/KamilarAndCooperData.csv"
d <- read.csv(f)
head(d)
```

## Creating simple linear regression model for MaxLongevity_m and Brain_Size_Species_Mean.

``` {r}
d <- d %>%  mutate(
  log_MaxLongevity_m = log(MaxLongevity_m),
  log_Brain_Size_Species_Mean = log(Brain_Size_Species_Mean)
)

a <- lm(MaxLongevity_m ~ Brain_Size_Species_Mean, data = d)
b <- lm(log_MaxLongevity_m ~ log_Brain_Size_Species_Mean, data = d)

tidy(a)
summary(a)

tidy(b)
summary(b)
```

##  Scatterplots with superimposed lines, fitted model equation included.

```{r}
p1 <- ggplot(d, aes(y = MaxLongevity_m, x = Brain_Size_Species_Mean)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE) +
  annotate("text", label = "Y = 249.0 + 1.2 * X", size = 4, x = 100, y = 800)

p2 <- ggplot(d, aes(y = log_MaxLongevity_m, x = log_Brain_Size_Species_Mean)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y  ~ x, se = FALSE) +
  annotate("text", label = "log(Y) = 4.9 + 0.2 * log(X)", size = 4, x = 2.3, y = 6.5)

grid.arrange(p1, p2, nrow = 1)
```

## β1 does not equal 0 in either model (1.2 and 0.2) so H0 can be rejected in both cases. 
## Calculating 90% confidence intervals.
 
```{r}
alpha = 0.1

(CIA <- tidy(a, conf.int = TRUE, conf.level = 1 - alpha))
(CIB <- tidy(b, conf.int = TRUE, conf.level = 1 - alpha))
```

## Adding Confidence and prediction interval bands to plot.

``` {r}
cia <- predict(a, newdata = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean), 
               interval = "confidence", level = 1 - alpha)
cia <- data.frame(cia)
cia <- cbind(d$Brain_Size_Species_Mean, cia)
names(cia) <- c("Brain_Size_Species_Mean", "c.fit", "c.lwr", "c.upr")
pia <- predict(a, newdata = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean), 
               interval = "prediction", level = 1 - alpha)
pia <- data.frame(pia)
pia <- cbind(d$Brain_Size_Species_Mean, pia)
names(pia) <- c("Brain_Size_Species_Mean", "p.fit", "p.lwr", "p.upr")

cib <- predict(b, newdata = data.frame(log_Brain_Size_Species_Mean = d$log_Brain_Size_Species_Mean), 
               interval = "confidence", leve = 1 - alpha)
cib <- data.frame(cib)
cib <- cbind(d$log_Brain_Size_Species_Mean, cib)
names(cib) <- c("log_Brain_Size_Species_Mean", "c.fit", "c.lwr", "c.upr")
pib <- predict(b, newdata = data.frame(log_Brain_Size_Species_Mean = d$log_Brain_Size_Species_Mean), 
               interval = "prediction", level = 1 - alpha)
pib <- data.frame(pib)
pib <- cbind(d$log_Brain_Size_Species_Mean, pib)
names(pib) <- c("log_Brain_Size_Species_Mean", "p.fit", "p.lwr", "p.upr")


p1 <- ggplot(d, aes(y = MaxLongevity_m, x = Brain_Size_Species_Mean)) +
  geom_point() +
  annotate("text", label = "Y = 249.0 + 1.2 * X", size = 4, x = 100, y = 800) +
  geom_line(data = cia, aes(x = Brain_Size_Species_Mean, y = c.fit, color = "black")) +
  geom_line(data = cia, aes(x = Brain_Size_Species_Mean, y = c.lwr, color = "red")) +
  geom_line(data = cia, aes(x = Brain_Size_Species_Mean, y = c.upr, color = "red")) +
  geom_line(data = pia, aes(x = Brain_Size_Species_Mean, y = p.lwr, color = "purple")) +
  geom_line(data = pia, aes(x = Brain_Size_Species_Mean, y = p.upr, color = "purple")) +
  scale_color_hue(labels = c("Fit", "CI", "PI"))

p2 <- ggplot(d, aes(y = log_MaxLongevity_m, x = log_Brain_Size_Species_Mean)) +
  geom_point() +
    annotate("text", label = "log(Y) = 4.9 + 0.2 * log(X)", size = 4, x = 2.3, y = 6.5) +
  geom_line(data = cib, aes(x = log_Brain_Size_Species_Mean, y = c.fit, color = "black")) +
  geom_line(data = cib, aes(x = log_Brain_Size_Species_Mean, y = c.lwr, color = "red")) +
  geom_line(data = cib, aes(x = log_Brain_Size_Species_Mean, y = c.upr, color = "red")) +
  geom_line(data = pib, aes(x = log_Brain_Size_Species_Mean, y = p.lwr, color = "purple")) +
  geom_line(data = pib, aes(x = log_Brain_Size_Species_Mean, y = p.upr, color = "purple")) +
  scale_color_hue(labels = c("Fit", "CI", "PI"))

grid.arrange(p1, p2, nrow = 1)
```

## Point estimate and 90% prediction interval of longevity for a species whose brain weight is 750g. 
## This estimate should not be trusted because the observed brain weight of the species is outside of the range of values within the model.

``` {r}
cia_point <- predict(a,
               newdata = data.frame(Brain_Size_Species_Mean = 750),
               interval = "confidence", level = 1 - alpha
)
cia_point

pia_point <- predict(a,
                     newdata = data.frame(Brain_Size_Species_Mean = 750),
                     interval = "prediction", level = 1 - alpha
)
pia_point

cib_point <- predict(b,
                     newdata = data.frame(log_Brain_Size_Species_Mean = log(750)),
               interval = "confidence", level = 1 - alpha
)
exp(cib_point)

pib_point <- predict(b,
                     newdata = data.frame(log_Brain_Size_Species_Mean = log(750)),
                     interval = "confidence", level = 1 - alpha
)
exp(cib_point)
```

## Looking at the two models I thing that the log-log transformed model is a better fit for the data because the fit line has smaller residuals and the points are distributed more linearly. 

# Challenge 2

## Running linear regression of log(HomeRange_km2) and log(Body_mass_female_mean)

``` {r}

d <- d %>% 
  mutate(log_HomeRange_km2 = log(HomeRange_km2),
         log_Body_mass_female_mean = log(Body_mass_female_mean)
  )

c <- lm(log_HomeRange_km2 ~ log_Body_mass_female_mean, data = d)
c
```

## Bootstrap of 1000 samples from dataset

```{r}
bootstrapped <- d %>% 
  specify(log_HomeRange_km2 ~ log_Body_mass_female_mean) %>% 
  generate(reps = 1000, type = "bootstrap")

slope <- vector()
intercept <- vector()

for(i in unique(bootstrapped$replicate)) {
  x <- filter(bootstrapped, replicate == i)
  y <- lm(log_HomeRange_km2 ~ log_Body_mass_female_mean, data = x)
  slope[[i]] <- y$coefficients[2]
  intercept[[i]] <- y$coefficients[1]
}

bootstrapped_lm <- tibble(
  slope = slope, 
  intercept = intercept)
```

## Histograms for β0 and β1

```{r}
ggplot(bootstrapped_lm, aes(x = slope)) +
  geom_histogram()
ggplot(bootstrapped_lm, aes(x = intercept)) + 
  geom_histogram()
```

## Estimates for SE and 95% for each β coefficient 

```{r}
alpha <- 0.05
confidence_level <- 1 - alpha
p_lower <- alpha / 2
p_upper <- 1 - (alpha / 2)
degrees_of_freedom <- nrow(bootstrapped_lm) - 2
critical_value <- qt(p_upper, df = degrees_of_freedom)

bs_ci_slope <- bootstrapped_lm %>% 
  summarize(
    estimate = mean(slope),
    std.error = sd(slope),
    lower = estimate - std.error * critical_value,
    upper = estimate + std.error * critical_value,
    perm.lower = quantile(slope, p_lower),
    perm.upper = quantile(slope, p_upper)
  )
bs_ci_slope

bs_ci_intercept <- bootstrapped_lm %>% 
  summarize(
    estimate = mean(intercept),
    std.error = sd(intercept),
    lower = estimate - std.error * critical_value,
    upper = estimate + std.error * critical_value,
    perm.lower = quantile(intercept, p_lower),
    perm.upper = quantile(intercept, p_upper)
  )
bs_ci_intercept
```

## The lm() function found that SE = `r tidy(c)$std.error[2]` and CI = `r tidy(c, conf.int = TRUE, conf.level = 1 - 0.05)$conf.low[2]` - `r tidy(c, conf.int = TRUE, conf.level = 1 - 0.05)$conf.high[2]` for β1, and SE = `r tidy(c)$std.error[1]` and CI = `r tidy(c, conf.int = TRUE, conf.level = 1 - 0.05)$conf.low[1]` - `r tidy(c, conf.int = TRUE, conf.level = 1 - 0.05)$conf.high[1]` for β0. 
## The bootstrap method found that SE = `r bs_ci_slope$std.error` and CI = `r bs_ci_slope$perm.lower` - `r bs_ci_slope$perm.upper` for β1, and SE = `r bs_ci_intercept$std.error` and CI = `r bs_ci_intercept$perm.lower` - `r bs_ci_intercept$perm.upper` for β0 

# Challenge 3
## Creating the function
``` {r}
boot_lm <- function(d, model, conf.level = 0.95, reps = 1000) {
  d <- d %>% mutate(
      logHR = log(HomeRange_km2),
      logBM = log(Body_mass_female_mean),
      logDL = log(DayLength_km)
  )
  model <- as.formula(model)
  
  c <- lm(model, data = d)
  
  x <- d %>% 
    specify(model) %>% 
    generate(reps = reps, type = "bootstrap")
  
  slope <- vector()
  intercept <- vector()
  
  for(i in unique(x$replicate)) {
    z <- filter(x, replicate == i)
    y <- lm(model, data = z)
    slope[[i]] <- y$coefficients[2]
    intercept[[i]] <- y$coefficients[1]
  }
  
  bootstrapped_lm <- tibble(
    slope = slope, 
    intercept = intercept)
  bootstrapped_lm
  
  confidence_level <- conf.level
  alpha <- 1 - conf.level
  p_lower <- alpha / 2
  p_upper <- 1 - (alpha / 2)
  degrees_of_freedom <- nrow(bootstrapped_lm) - 2
  critical_value <- qt(p_upper, df = degrees_of_freedom)
  
  bootstrapped_lm <- bootstrapped_lm %>% 
    summarize(
      model = "boot",
      beta1 = mean(slope), 
      beta1.std.error = sd(slope),
      beta1.ci.lower = quantile(slope, p_lower),
      beta1.ci.upper = quantile(slope, p_upper),
      beta0 = mean(intercept),
      beta0.std.error = sd(intercept),
      beta0.ci.lower = quantile(intercept, p_lower),
      beta0.ci.upper = quantile(intercept, p_upper),
      
    )
  lm_model <- tibble(model = "lm()",
                     beta1 = c$coefficients[2],
                     beta1.std.error = tidy(c)$std.error[2],
                     beta1.ci.lower =  tidy(c, conf.int = TRUE, conf.level = conf.level)$conf.low[2],
                     beta1.ci.upper =  tidy(c, conf.int = TRUE, conf.level = conf.level)$conf.high[2],
                     beta0 = c$coefficients[1],
                     beta0.std.error = tidy(c)$std.error[1],
                     beta0.ci.lower = tidy(c, conf.int = TRUE, conf.level = conf.level)$conf.low[1],
                     beta0.ci.upper = tidy(c, conf.int = TRUE, conf.level = conf.level)$conf.high[1]
  )
  bootstrapped_lm <- rbind(lm_model, bootstrapped_lm)
  bootstrapped_lm
}
```

## Running the function on: 
###-log(HomeRange_km2) ~ log(Body_mass_female_mean)
###-log(DayLength_km) ~ log(Body_mass_female_mean)
###-log(HomeRange_km2) ~ log(Body_mass_female_mean) + MeanGroupSize
```{r}
boot_lm(d = d, model = "logHR ~ logBM")
boot_lm(d = d, model = "logDL ~ logBM")
```